{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matadan11/Medicion-Latencia-Homework/blob/main/Tarea3_MPI_Explicada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPjqupmQYKRK"
      },
      "source": [
        "# üß™ Tarea 3: Comunicaci√≥n colectiva y medici√≥n de latencia con MPI en Python (Colab)\n",
        "\n",
        "**Autor:** Daniel Matarrita  \n",
        "**Curso:** Computaci√≥n Paralela y Distribuida  \n",
        "**Profesor:** Johansell Villalobos  \n",
        "\n",
        "Este notebook implementa y explica paso a paso las dos partes de la Tarea 3:\n",
        "- Parte A: Operaciones colectivas en MPI (`Bcast`, `Scatter`, `Reduce`)\n",
        "- Parte B: Medici√≥n de latencia punto a punto (`Send`, `Recv`)\n",
        "\n",
        "> ‚ö†Ô∏è Este notebook est√° pensado para ejecutarse en **Google Colab**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya1zNxnNYKRR",
        "outputId": "3b6f4f00-9a75-4208-9fe5-fbc13d232506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Paso 1: Instalaci√≥n de dependencias necesarias en Colab\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install -y libopenmpi-dev openmpi-bin > /dev/null\n",
        "!pip install mpi4py > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtYuCVd-YKRT"
      },
      "source": [
        "## üß© Parte A: Estad√≠sticas globales con operaciones colectivas\n",
        "\n",
        "En esta parte usaremos `MPI_Bcast`, `MPI_Scatter`, y `MPI_Reduce` para distribuir un arreglo de n√∫meros y calcular el m√≠nimo, m√°ximo y promedio global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi8B6thJYKRV"
      },
      "outputs": [],
      "source": [
        "%%writefile parteA_estadisticas_mpi.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Tama√±o del arreglo desde argumentos\n",
        "N = int(sys.argv[1]) if len(sys.argv) > 1 else 1000000\n",
        "\n",
        "if N % size != 0:\n",
        "    if rank == 0:\n",
        "        print(\"Error: N no divisible entre procesos\")\n",
        "    exit()\n",
        "\n",
        "sub_size = N // size\n",
        "data = None\n",
        "if rank == 0:\n",
        "    data = np.random.uniform(0, 100, N)\n",
        "\n",
        "sub_data = np.empty(sub_size, dtype='d')\n",
        "comm.Scatter(data, sub_data, root=0)\n",
        "\n",
        "local_min = np.min(sub_data)\n",
        "local_max = np.max(sub_data)\n",
        "local_sum = np.sum(sub_data)\n",
        "\n",
        "global_min = comm.reduce(local_min, op=MPI.MIN, root=0)\n",
        "global_max = comm.reduce(local_max, op=MPI.MAX, root=0)\n",
        "global_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"Min global: {global_min}\")\n",
        "    print(f\"Max global: {global_max}\")\n",
        "    print(f\"Promedio global: {global_sum / N}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Mg-60wYKRW"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è Ejecutar Parte A con 4 procesos\n",
        "!mpirun -np 4 python3 parteA_estadisticas_mpi.py 1000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJAZRW4JYKRX"
      },
      "source": [
        "## üì° Parte B: Medici√≥n de latencia punto a punto\n",
        "\n",
        "Esta parte implementa la medici√≥n de latencia entre dos procesos usando `MPI_Send` y `MPI_Recv` de ida y vuelta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpDRepkYYKRY"
      },
      "outputs": [],
      "source": [
        "%%writefile parteB_latencia_mpi.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if size != 2:\n",
        "    if rank == 0:\n",
        "        print(\"Este programa requiere exactamente 2 procesos\")\n",
        "    exit()\n",
        "\n",
        "N = 10000\n",
        "msg = np.array([0], dtype='b')  # Mensaje de 1 byte\n",
        "\n",
        "if rank == 0:\n",
        "    start = MPI.Wtime()\n",
        "    for _ in range(N):\n",
        "        comm.Send([msg, MPI.BYTE], dest=1)\n",
        "        comm.Recv([msg, MPI.BYTE], source=1)\n",
        "    end = MPI.Wtime()\n",
        "    total_time = end - start\n",
        "    avg_latency = (total_time / N) * 1e6  # microsegundos\n",
        "    print(f\"Mensaje de 1 byte transmitido {N} veces.\")\n",
        "    print(f\"Latencia promedio (ida y vuelta): {avg_latency:.2f} ¬µs\")\n",
        "    print(f\"Latencia estimada unidireccional: {avg_latency/2:.2f} ¬µs\")\n",
        "\n",
        "elif rank == 1:\n",
        "    for _ in range(N):\n",
        "        comm.Recv([msg, MPI.BYTE], source=0)\n",
        "        comm.Send([msg, MPI.BYTE], dest=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWqrmMoiYKRZ"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è Ejecutar Parte B con 2 procesos\n",
        "!mpirun -np 2 python3 parteB_latencia_mpi.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}